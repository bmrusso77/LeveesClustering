# -*- coding: utf-8 -*-
"""Copy of C1. Clustering_GI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AkqdF6ShDYwfbxuFN8TuF0FgMHH_p6jJ

# Clustering analysis - Grand Island
##### Author: Brittany M. Russo
##### Email: bmrusso@berkeley.edu
##### University of California, Berkeley
##### Geosystems Engineering
##### Date Created: June 20, 2023
##### Date Modified:
##### Purpose: This codes performs three clustering algorithms on the data and then saves the clusters to a csv for analysis later on. Clustering analyses: k-means, hierarchical, DBSCAN.
# ---------------------------------------------------------------------

# Importing Libraries and Bringing in Data
"""

# -----------------------------------------------------------------------------
#                            IMPORTING LIBRARIES
# -----------------------------------------------------------------------------
from google.colab import files
import io
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import griddata
from sklearn.metrics import r2_score
from scipy.stats import pearsonr
from numpy import cov
from numpy.random import seed
from numpy.random import randn
from numpy.random import normal
from scipy.stats import ttest_ind
from sklearn import linear_model
import statsmodels.api as sm
from sklearn.metrics.pairwise import pairwise_distances_argmin
import random
from sklearn.metrics.pairwise import pairwise_distances_argmin_min
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)
from tabulate import tabulate
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
import matplotlib as mpl
# -----------------------------------------------------------------------------

# Using code from: milesgranger
# https://github.com/milesgranger/gap_statistic
!pip install git+https://github.com/milesgranger/gap_statistic.git
from gap_statistic import OptimalK

"""# User Defined Function"""

# ----- STANDARDIZATION -------------------------------------------------------
# Standardize User Defined Function
def standardize(X):
  "standardizes a set of data where X is the data in the form (n_samples,"
  "n_variables"
  return ((X - np.mean(X, axis = 0)) / np.std(X, axis = 0))
# -----------------------------------------------------------------------------

# ---- DBSCAN ELBOW -----------------------------------------------------------
def db_elbow(X, n):
  "Calculates the distances needed using Nearest Neighbor to plot the elbow"
  "Graph to determine the optimal epsilon for the DBSCAN clustering analysis"
  "Inputs: X is the data in (n_samples, n_variables) and n is the chosen minPts"
  "Distances is used for plotting"
  # Using the Nearest Neighbor algorithm to calculate the distance between each
  # point in the data set
  neighbors = NearestNeighbors(n_neighbors=n)
  neighbors_fit = neighbors.fit(X)
  distances, indices = neighbors_fit.kneighbors(X)

  # Sorting the distances in ascending order for plotting purposes
  distances = np.sort(distances, axis=0)
  distances = distances[:,1]
  return distances
# -----------------------------------------------------------------------------

"""# Uploading Data"""

# ----- Bringing Data In ------------------------------------------------------
# importing csv data
uploaded = files.upload()
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
#                             DATA INITIALIZATION
# .............................................................................
# Code Outline:
# (1) Raw Data Visualization
# (2) Data manipulation - creating data variables for the different analyses
# (3) Clustering with Raw Data
# (4) Clustering with Standardized Data
# -----------------------------------------------------------------------------
# This reads the inputted CSV files and puts that data into data frames
# -----------------------------------------------------------------------------
# putting the data into pandas dataframes
data_raw = pd.read_csv(io.BytesIO(uploaded['BI_south_df.csv']))
# ----- End of Section --------------------------------------------------------

"""# Visualization"""

# (1) Raw Data Visualization --------------------------------------------------
plt.scatter(data_raw['vs_mps'], data_raw['r_ohmm'], c=data_raw['Y_m'], cmap='magma')
plt.colorbar(label='UTM Y Coordinate (m)')
plt.xlabel('Shear Wave Velocity (m/s)')
plt.ylabel('Electrical Resistivity (Î©m)')
# plt.title('Colored by depth')
# -----------------------------------------------------------------------------

# Plotting spatially
plt.scatter(data_raw['X_m'], data_raw['elev_m'], c=data_raw['Wat_Head_m'], cmap='magma')
plt.colorbar(label='Water Total Head (m)')
plt.xlabel('UTM Zone 10N X-Coordinate (m)')
plt.ylabel('Elevation (m)')

"""# Raw Analysis"""

data_raw

# ----- Standardizing the Data ------------------------------------------------
x = data_raw['X_m']
# y = data_raw['Y_m']
z = data_raw['elev_m']
vs = data_raw['vs_mps']
r = data_raw['r_ohmm']

# ----- Feature Selection Process ---------------------------------------------
# this needs to be looked at since this is spatial data
# (1) Just Vs and R
df_vr = pd.DataFrame({'vs_mps':vs, 'r_ohmm':r})

# (2) With X and Z
# df_all = pd.DataFrame({'vs_mps':vs, 'r_ohmm':r, 'x_m':x, 'z_m':z})
# -----------------------------------------------------------------------------

"""## K-Means and Hierarchical

"""

# (1) Elbow Rule and Silhouette Rule
# ----- Looping using Kmeans --------------------------------------------------
k = range(2, 20)          # number of clusters for analysis

# kmeans loop
wcss = []
silhouette_avg = []
for i in k:
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10,
                    random_state=252)
    kmeans.fit(df_vr)
    wcss.append(kmeans.inertia_)
    # silhouette score
    silhouette_avg.append(silhouette_score(df_vr, kmeans.labels_, metric='euclidean'))

# ----- Plotting --------------------------------------------------------------
# Creating the plot
fig = plt.figure(figsize=(10, 4))
ax = fig.add_subplot(121)
plt.plot(range(2,20), wcss, 'k', marker='o')

# Formatting the plot
plt.title('Elbow Rule Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
# plt.xlim([0,20])
# ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
#                which="both", direction='in')
# ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
#                labelright=False)
ax.xaxis.set_minor_locator(MultipleLocator(1))
# ax.yaxis.set_minor_locator(MultipleLocator(500000000))

# -----------------------------------------------------------------------------
# Creating the plot
ax = fig.add_subplot(122)
plt.plot(range(2,20), silhouette_avg, 'k', marker='o')

# Formatting the plot
plt.title('Silhouette Average Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Average')
# ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
#                which="both", direction='in')
# ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
#                labelright=False)
# plt.xlim([1,20])
ax.xaxis.set_minor_locator(MultipleLocator(1))
# ax.yaxis.set_minor_locator(MultipleLocator(500000000))
r = range(2, 20)
print('The Max Silhouette, Cluster=', r[np.argmax(silhouette_avg)])
# -----------------------------------------------------------------------------

# (2) Gap Statistics

# ----- Performing the Analysis -----------------------------------------------
gs = OptimalK(n_jobs=1, n_iter=1000)

n_clusters = gs(df_vr, n_refs=10, cluster_array=np.arange(2,20))

# ----- Creating the plot -----------------------------------------------------
fig = plt.figure(figsize=(20, 5))
ax = fig.add_subplot(131)

# Creating the First Plot
plt.plot(gs.gap_df['n_clusters'], gs.gap_df['gap_value'], 'k', marker='o')
plt.title('Gap Statistics Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap Value')
ax.xaxis.set_minor_locator(MultipleLocator(1))
print('Cluster of Max Gap = ', r[np.argmax(gs.gap_df['gap_value'])])

# Creating the Second Plot
ax2 = fig.add_subplot(132)

plt.plot(gs.gap_df['n_clusters'], gs.gap_df['diff'], 'k', marker='o')
plt.title('Diff Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Diff')
ax.xaxis.set_minor_locator(MultipleLocator(1))

# Creating the third Plot
ax2 = fig.add_subplot(133)

plt.plot(gs.gap_df['n_clusters'], gs.gap_df['gap*'], 'k', marker='o')
plt.title('Gap* Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap* Value')
ax.xaxis.set_minor_locator(MultipleLocator(1))
print('Cluster of Max Gap* = ', r[np.argmax(gs.gap_df['gap*'])])
# -----------------------------------------------------------------------------

# (4) K-Means Clustering Algorithm

# ----- Applying SKLEARN algorithm --------------------------------------------
# defining the number of clusters
k = 6

# applying the alogrithm
kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=500, n_init=10,
                    random_state=252)
k_labels_raw = kmeans.fit_predict(df_vr)
# -----------------------------------------------------------------------------
# (5) Hierarchical Clustering

# ----- Hierarchical Clustering Algorithm -------------------------------------
hierarchical_cluster = AgglomerativeClustering(n_clusters=k,
                                               metric='euclidean', linkage='ward')
h_labels_raw = hierarchical_cluster.fit_predict(df_vr)

# -----------------------------------------------------------------------------

"""## DBSCAN"""

# (6) DBSCAN Elbow Rule
# ----- DBSCAN Elbow Rule -----------------------------------------------------
# Determining MinPts
mp = 4                                      # dim = 2

# Generating Distances
distances = db_elbow(df_vr, mp)

# Creating the graph
fig = plt.figure(figsize=(5, 4))
ax = fig.add_subplot(111)

# Plotting the Data
plt.plot(distances, 'k')

# Formatting the Figure
plt.title('Elbow Rule (DBSCAN)',fontsize=14)
plt.xlabel('Data Points sorted by Distance',fontsize=14)
plt.ylabel('Îµ',fontsize=16)
ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
               which="both", direction='in')
ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
                labelright=False)
# ax.yaxis.set_minor_locator(MultipleLocator(0.2))
# ax.xaxis.set_minor_locator(MultipleLocator(10))
## Formatting that needs to be commented
# plt.xlim([300, 450])
# plt.ylim([0, 500])
ax.yaxis.set_minor_locator(MultipleLocator(1))
# plt.savefig("elbow_dbscan.png",dpi=300)
# -----------------------------------------------------------------------------

# (7) Performing the DBSCAN Analysis
  # Performing the DBSCAN clustering algorithm
e = 3
minpts = 4
dbscan_clust = DBSCAN(eps=e, min_samples=minpts).fit(df_vr)

# Getting the cluster labels
d_labels_raw = dbscan_clust.labels_

# -----------------------------------------------------------------------------
# ----- Saving the new labels to the dataFrame and exporting to CSV -----------
# Copying the DataFrames first
data = data_raw.copy()

# adding the data to the dataframe
data['KMeans'] = k_labels_raw
data['Hierarchical'] = h_labels_raw
data['DBSCAN'] = d_labels_raw

# Saving the DataFrame
data.to_csv('clustered_site3_raw.csv', index=False)
# -----------------------------------------------------------------------------

"""# Standardized Analysis"""

# ----- Standardizing the Data ------------------------------------------------
data_std = standardize(data_raw)
x = data_std['X_m']
# y = data_std['elev_m']
z = data_std['elev_m']
vs = data_std['vs_mps']
r = data_std['r_ohmm']

# ----- Feature Selection Process ---------------------------------------------
# this needs to be looked at since this is spatial data
# Just Vs and R
# df_vr = pd.DataFrame({'vs_mps':vs})
# df_vr = pd.DataFrame({'r_ohmm':r})
df_vr = pd.DataFrame({'vs_mps':vs, 'r_ohmm':r})
# -----------------------------------------------------------------------------

"""## K-Means and Hierarchical"""

# (1) Elbow Rule and Silhouette Rule
# ----- Looping using Kmeans --------------------------------------------------
k = range(2, 20)          # number of clusters for analysis

# kmeans loop
wcss = []
silhouette_avg = []
for i in k:
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10,
                    random_state=252)
    kmeans.fit(df_vr)
    wcss.append(kmeans.inertia_)
    # silhouette score
    silhouette_avg.append(silhouette_score(df_vr, kmeans.labels_, metric='euclidean'))

# ----- Plotting --------------------------------------------------------------
# Creating the plot
fig = plt.figure(figsize=(10, 4))
ax = fig.add_subplot(121)
plt.plot(range(2,20), wcss, 'k', marker='o')

# Formatting the plot
plt.title('Elbow Rule Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
               which="both", direction='in')
ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
               labelright=False)
ax.xaxis.set_minor_locator(MultipleLocator(0.5))
# ax.yaxis.set_minor_locator(MultipleLocator(500000000))

# -----------------------------------------------------------------------------
# Creating the plot
ax = fig.add_subplot(122)
plt.plot(range(2,20), silhouette_avg, 'k', marker='o')
r = range(2,20)

# Formatting the plot
plt.title('Silhouette Average Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Average')
ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
               which="both", direction='in')
ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
               labelright=False)
ax.xaxis.set_minor_locator(MultipleLocator(0.5))
# ax.yaxis.set_minor_locator(MultipleLocator(500000000))
print('The Max Silhouette, Cluster=', r[np.argmax(silhouette_avg)])
# -----------------------------------------------------------------------------

# (2) Gap Statistics

# ----- Performing the Analysis -----------------------------------------------
gs = OptimalK(n_jobs=1, n_iter=1000)

n_clusters = gs(df_vr, n_refs=10, cluster_array=np.arange(2,20))

# ----- Creating the plot -----------------------------------------------------
fig = plt.figure(figsize=(20, 5))
ax = fig.add_subplot(131)

# Creating the First Plot
plt.plot(gs.gap_df['n_clusters'], gs.gap_df['gap_value'], 'k', marker='o')
plt.title('Gap Statistics Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap Value')
print('Cluster of Max Gap = ', r[np.argmax(gs.gap_df['gap_value'])])

# Creating the Second Plot
ax2 = fig.add_subplot(132)

plt.plot(gs.gap_df['n_clusters'], gs.gap_df['diff'], 'k', marker='o')
plt.title('Diff Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Diff')
print(gs.gap_df['diff'])
# Creating the third Plot
ax2 = fig.add_subplot(133)

plt.plot(gs.gap_df['n_clusters'], gs.gap_df['gap*'], 'k', marker='o')
plt.title('Gap* Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Gap* Value')
print('Cluster of Max Gap* = ', r[np.argmax(gs.gap_df['gap*'])])
# -----------------------------------------------------------------------------

# (4) K-Means Clustering Algorithm

# ----- Applying SKLEARN algorithm --------------------------------------------
# defining the number of clusters
k = 3

# applying the alogrithm
kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=500, n_init=10,
                    random_state=252)
k_labels_std = kmeans.fit_predict(df_vr)
# -----------------------------------------------------------------------------
# (5) Hierarchical Clustering

# ----- Hierarchical Clustering Algorithm -------------------------------------
hierarchical_cluster = AgglomerativeClustering(n_clusters=k,
                                               metric ='euclidean', linkage='ward')
h_labels_std = hierarchical_cluster.fit_predict(df_vr)

# -----------------------------------------------------------------------------
# ----- Saving the new labels to the dataFrame and exporting to CSV -----------
# Copying the DataFrames first
data_std = data_raw.copy()

# adding the data to the dataframe
data_std['KMeans'] = k_labels_std
data_std['Hierarchical'] = h_labels_std
# data_std['DBSCAN'] = d_labels_std

# Saving the DataFrame
# change the name to include the survey being analyzed
data_std.to_csv('clustering_south.csv', index=False)
# -----------------------------------------------------------------------------

"""## DBSCAN"""

# # (6) DBSCAN Elbow Rule
# # ----- DBSCAN Elbow Rule -----------------------------------------------------
# # Determining MinPts
# mp = 4                                      # dim = 2

# # Generating Distances
# distances = db_elbow(df_vr, mp)

# # Creating the graph
# fig = plt.figure(figsize=(5, 4))
# ax = fig.add_subplot(111)

# # Plotting the Data
# plt.plot(distances, 'k')

# # Formatting the Figure
# plt.title('Elbow Rule (DBSCAN)',fontsize=14)
# plt.xlabel('Data Points sorted by Distance',fontsize=14)
# plt.ylabel('Îµ',fontsize=16)
# ax.tick_params(bottom=True, top=True, left=True, right=True, axis="both",
#                which="both", direction='in')
# ax.tick_params(labelbottom=True, labeltop=False, labelleft=True,
#                 labelright=False)
# ax.yaxis.set_minor_locator(MultipleLocator(0.2))
# ax.xaxis.set_minor_locator(MultipleLocator(10))
# ## Formatting that needs to be commented
# # plt.xlim([300, 450])
# # plt.ylim([0, 500])
# ax.yaxis.set_minor_locator(MultipleLocator(0.01))
# # plt.savefig("elbow_dbscan.png",dpi=300)
# # -----------------------------------------------------------------------------

# (7) Performing the DBSCAN Analysis
  # Performing the DBSCAN clustering algorithm
# e = 0.07
# minpts = 4
# dbscan_clust = DBSCAN(eps=e, min_samples=minpts).fit(df_vr)

# # Getting the cluster labels
# d_labels_std = dbscan_clust.labels_

# -----------------------------------------------------------------------------
# ----- Saving the new labels to the dataFrame and exporting to CSV -----------
# Copying the DataFrames first
data_std = data_raw.copy()

# adding the data to the dataframe
data_std['KMeans'] = k_labels_std
data_std['Hierarchical'] = h_labels_std
# data_std['DBSCAN'] = d_labels_std

# Saving the DataFrame
# change the name to include the survey being analyzed
data_std.to_csv('clustering_fc3_site1_R.csv', index=False)
# -----------------------------------------------------------------------------

"""# End of Code"""